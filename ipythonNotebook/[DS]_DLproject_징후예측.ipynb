{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import koreanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import glob\n",
    "# 데이터 전처리 후 시각화에 도움을 주는 라이브러리\n",
    "import missingno as msno\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion, make_union\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import collections\n",
    "import imblearn\n",
    "from imblearn.over_sampling import ADASYN, RandomOverSampler, SMOTENC, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "import os\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.stats import chi2_contingency, ttest_ind\n",
    "import category_encoders as CE\n",
    "\n",
    "# import eli5\n",
    "# from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# for dirname, _, filenames in os.walk('../kaggle/MLproject_dataset'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 말뭉치 데이터셋을 준비합니다. texts는 입력 텍스트 데이터, labels는 해당 텍스트에 대한 레이블입니다.\n",
    "# 2. 텍스트 데이터를 전처리합니다. Tokenizer를 사용하여 텍스트를 숫자 시퀀스로 변환하고, 패딩을 추가하여 시퀀스의 길이를 동일하게 맞춥니다.\n",
    "# 3. 레이블 데이터를 전처리합니다. 레이블을 범주형 형식으로 변환합니다.\n",
    "# 4. 모델을 구성합니다. Embedding 레이어로 단어 임베딩을 적용하고, LSTM 레이어를 사용하여 시퀀스 데이터를 처리합니다.\n",
    "# 5. 모델을 컴파일합니다. 손실 함수와 옵티마이저를 지정합니다.\n",
    "# 6. 모델을 훈련합니다. 입력 데이터와 레이블 데이터를 사용하여 모델을 학습시킵니다.\n",
    "# 7. 예측을 수행합니다. 새로운 텍스트 데이터를 전처리한 후, 학습된 모델을 사용하여 예측값을 얻습니다.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 말뭉치 데이터셋 준비\n",
    "texts = [...]  # 말뭉치 데이터셋 (입력 텍스트)\n",
    "labels = [...]  # 말뭉치에 대한 레이블 (출력 예측값)\n",
    "\n",
    "# 텍스트 데이터 전처리\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "input_data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# 레이블 데이터 전처리\n",
    "label_data = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# 모델 구성\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(input_data, label_data, epochs=10, batch_size=32)\n",
    "\n",
    "# 예측하기\n",
    "new_texts = [...]  # 예측할 텍스트 데이터\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_data = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
    "predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fever</th>\n",
       "      <th>Tiredness</th>\n",
       "      <th>Dry-Cough</th>\n",
       "      <th>Difficulty-in-Breathing</th>\n",
       "      <th>Sore-Throat</th>\n",
       "      <th>None_Sympton</th>\n",
       "      <th>Pains</th>\n",
       "      <th>Nasal-Congestion</th>\n",
       "      <th>Runny-Nose</th>\n",
       "      <th>Diarrhea</th>\n",
       "      <th>None_Experiencing</th>\n",
       "      <th>Age_0-9</th>\n",
       "      <th>Age_10-19</th>\n",
       "      <th>Age_20-24</th>\n",
       "      <th>Age_25-59</th>\n",
       "      <th>Age_60+</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Gender_Transgender</th>\n",
       "      <th>Severity_Mild</th>\n",
       "      <th>Severity_Moderate</th>\n",
       "      <th>Severity_None</th>\n",
       "      <th>Severity_Severe</th>\n",
       "      <th>Contact_Dont-Know</th>\n",
       "      <th>Contact_No</th>\n",
       "      <th>Contact_Yes</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316796</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316797</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316798</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316799</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316800 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Fever  Tiredness  Dry-Cough  Difficulty-in-Breathing  Sore-Throat  \\\n",
       "0           1          1          1                        1            1   \n",
       "1           1          1          1                        1            1   \n",
       "2           1          1          1                        1            1   \n",
       "3           1          1          1                        1            1   \n",
       "4           1          1          1                        1            1   \n",
       "...       ...        ...        ...                      ...          ...   \n",
       "316795      0          0          0                        0            0   \n",
       "316796      0          0          0                        0            0   \n",
       "316797      0          0          0                        0            0   \n",
       "316798      0          0          0                        0            0   \n",
       "316799      0          0          0                        0            0   \n",
       "\n",
       "        None_Sympton  Pains  Nasal-Congestion  Runny-Nose  Diarrhea  \\\n",
       "0                  0      1                 1           1         1   \n",
       "1                  0      1                 1           1         1   \n",
       "2                  0      1                 1           1         1   \n",
       "3                  0      1                 1           1         1   \n",
       "4                  0      1                 1           1         1   \n",
       "...              ...    ...               ...         ...       ...   \n",
       "316795             1      0                 0           0         0   \n",
       "316796             1      0                 0           0         0   \n",
       "316797             1      0                 0           0         0   \n",
       "316798             1      0                 0           0         0   \n",
       "316799             1      0                 0           0         0   \n",
       "\n",
       "        None_Experiencing  Age_0-9  Age_10-19  Age_20-24  Age_25-59  Age_60+  \\\n",
       "0                       0        1          0          0          0        0   \n",
       "1                       0        1          0          0          0        0   \n",
       "2                       0        1          0          0          0        0   \n",
       "3                       0        1          0          0          0        0   \n",
       "4                       0        1          0          0          0        0   \n",
       "...                   ...      ...        ...        ...        ...      ...   \n",
       "316795                  1        0          0          0          0        1   \n",
       "316796                  1        0          0          0          0        1   \n",
       "316797                  1        0          0          0          0        1   \n",
       "316798                  1        0          0          0          0        1   \n",
       "316799                  1        0          0          0          0        1   \n",
       "\n",
       "        Gender_Female  Gender_Male  Gender_Transgender  Severity_Mild  \\\n",
       "0                   0            1                   0              1   \n",
       "1                   0            1                   0              1   \n",
       "2                   0            1                   0              1   \n",
       "3                   0            1                   0              0   \n",
       "4                   0            1                   0              0   \n",
       "...               ...          ...                 ...            ...   \n",
       "316795              0            0                   1              0   \n",
       "316796              0            0                   1              0   \n",
       "316797              0            0                   1              0   \n",
       "316798              0            0                   1              0   \n",
       "316799              0            0                   1              0   \n",
       "\n",
       "        Severity_Moderate  Severity_None  Severity_Severe  Contact_Dont-Know  \\\n",
       "0                       0              0                0                  0   \n",
       "1                       0              0                0                  0   \n",
       "2                       0              0                0                  1   \n",
       "3                       1              0                0                  0   \n",
       "4                       1              0                0                  0   \n",
       "...                   ...            ...              ...                ...   \n",
       "316795                  0              0                1                  0   \n",
       "316796                  0              0                1                  1   \n",
       "316797                  0              1                0                  0   \n",
       "316798                  0              1                0                  0   \n",
       "316799                  0              1                0                  1   \n",
       "\n",
       "        Contact_No  Contact_Yes Country  \n",
       "0                0            1   China  \n",
       "1                1            0   China  \n",
       "2                0            0   China  \n",
       "3                0            1   China  \n",
       "4                1            0   China  \n",
       "...            ...          ...     ...  \n",
       "316795           1            0   Other  \n",
       "316796           0            0   Other  \n",
       "316797           0            1   Other  \n",
       "316798           1            0   Other  \n",
       "316799           0            0   Other  \n",
       "\n",
       "[316800 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/minsoo/Downloads/archive/Cleaned-Data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fever                      0\n",
       "Tiredness                  0\n",
       "Dry-Cough                  0\n",
       "Difficulty-in-Breathing    0\n",
       "Sore-Throat                0\n",
       "None_Sympton               0\n",
       "Pains                      0\n",
       "Nasal-Congestion           0\n",
       "Runny-Nose                 0\n",
       "Diarrhea                   0\n",
       "None_Experiencing          0\n",
       "Age_0-9                    0\n",
       "Age_10-19                  0\n",
       "Age_20-24                  0\n",
       "Age_25-59                  0\n",
       "Age_60+                    0\n",
       "Gender_Female              0\n",
       "Gender_Male                0\n",
       "Gender_Transgender         0\n",
       "Severity_Mild              0\n",
       "Severity_Moderate          0\n",
       "Severity_None              0\n",
       "Severity_Severe            0\n",
       "Contact_Dont-Know          0\n",
       "Contact_No                 0\n",
       "Contact_Yes                0\n",
       "Country                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 의료 전문서적 말뭉치 데이터셋이 여러 개의 txt 파일로 구성되어 있는 경우, 데이터셋을 효과적으로 학습시키기 위해 몇 가지 접근 방법을 고려할 수 있습니다. \n",
    "# 여기에는 데이터 로드, 전처리, 통합 등이 포함될 수 있습니다. 다음은 이러한 상황에서의 일반적인 절차를 안내해 드리겠습니다:\n",
    "\n",
    "# 데이터 로드: 각각의 txt 파일을 개별적으로 로드합니다. 이를 위해 파일 경로를 지정하고 파일을 읽어 텍스트 데이터를 추출하는 방법을 사용할 수 있습니다.\n",
    "\n",
    "# 데이터 전처리: 로드한 텍스트 데이터에 대해 필요한 전처리 단계를 수행합니다. 이는 텍스트 정제, 토큰화, 불용어 제거, 단어 임베딩 등을 포함할 수 있습니다. \n",
    "# 전처리 과정은 데이터의 특성과 목적에 따라 다르게 수행될 수 있습니다.\n",
    "\n",
    "# 데이터 통합: 여러 개의 txt 파일로 구성된 데이터셋을 통합합니다. 이를 위해 로드하고 전처리한 데이터를 하나의 리스트로 결합합니다. \n",
    "# 예를 들어, 리스트로 각각의 txt 파일 데이터를 저장하고, 이들을 합쳐 전체 데이터셋을 구성할 수 있습니다.\n",
    "\n",
    "# 모델 학습: 전처리 및 통합된 데이터셋을 사용하여 모델을 학습시킵니다. 적절한 딥러닝 모델 아키텍처를 선택하고, 입력 데이터와 출력 예측값을 사용하여 모델을 훈련시킬 수 있습니다.\n",
    "\n",
    "# 예시 코드는 다음과 같이 구성될 수 있습니다:\n",
    "\n",
    "import os\n",
    "\n",
    "# 데이터 로드\n",
    "data_directory = \"data_directory\"  # 데이터가 있는 디렉토리 경로\n",
    "file_names = os.listdir(data_directory)\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_directory, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        texts.append(text)\n",
    "        labels.append(file_name)  # 필요에 따라 레이블을 지정할 수 있음\n",
    "\n",
    "# 데이터 전처리\n",
    "# ...\n",
    "\n",
    "# 데이터 통합\n",
    "combined_texts = texts\n",
    "combined_labels = labels\n",
    "\n",
    "# 모델 학습\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '재활의학/물리치료학/작업치료학 척추부, 근육, 받침대, 온열치료, 척추'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f0/jc1nd34d035_12y6rftwr4600000gn/T/ipykernel_65120/812756878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# 레이블 데이터 전처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mlabel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# 모델 구성, 컴파일, 훈련은 이전 예시 코드와 동일하게 진행하면 됩니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '재활의학/물리치료학/작업치료학 척추부, 근육, 받침대, 온열치료, 척추'"
     ]
    }
   ],
   "source": [
    "# 위의 코드에서는 다음과 같은 단계를 거칩니다:\n",
    "\n",
    "# 말뭉치 데이터셋을 준비합니다. texts는 입력 텍스트 데이터, labels는 해당 텍스트에 대한 레이블입니다.\n",
    "# AutoTokenizer를 사용하여 원하는 토크나이저를 자동으로 로드합니다. 위 예시에서는 \"bert-base-uncased\" 모델의 토크나이저를 사용하도록 설정하였습니다.\n",
    "# 텍스트 데이터를 전처리하고 토큰화합니다. tokenizer 객체를 사용하여 텍스트를 토큰화하고, 패딩과 자르기(truncation)를 적용하여 일관된 길이로 맞춥니다. return_tensors 매개변수를 통해 TensorFlow에서 사용할 수 있는 텐서 형태로 출력합니다.\n",
    "# 레이블 데이터를 전처리합니다. 레이블을 범주형 형식으로 변환합니다.\n",
    "# 모델 구성, 컴파일, 훈련은 이전 예시 코드와 동일하게 진행하면 됩니다.\n",
    "# 예측을 수행합니다. 새로운 텍스트 데이터를 전처리하고 토큰화한 후, 학습된 모델을 사용하여 예측값을 얻습니다.\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 말뭉치 데이터셋 준비\n",
    "# JSON 형식의 학습 데이터셋 로드\n",
    "with open('/Users/minsoo/Downloads/154.의료, 법률 전문 서적 말뭉치/01.데이터/Training/02.라벨링데이터/Training_medical.json', 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# 학습 데이터셋 전처리\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for data in dataset['data']:\n",
    "    text = data['text']\n",
    "    # label = data[['category', 'keyword']]\n",
    "    category = data['category']\n",
    "    keywords = data['keyword']\n",
    "    label = f\"{category} {', '.join(keywords)}\"\n",
    "    texts.append(text)\n",
    "    labels.append(label)\n",
    "\n",
    "# AutoTokenizer 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 텍스트 데이터 전처리 및 토큰화\n",
    "input_data = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"tf\")\n",
    "\n",
    "# 레이블 데이터 전처리\n",
    "label_data = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# 모델 구성, 컴파일, 훈련은 이전 예시 코드와 동일하게 진행하면 됩니다.\n",
    "\n",
    "# 모델 구성\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(input_data, label_data, epochs=10, batch_size=32)\n",
    "\n",
    "# 예측하기\n",
    "new_texts = [...]  # 예측할 텍스트 데이터\n",
    "\n",
    "# 텍스트 데이터 전처리 및 토큰화\n",
    "new_data = tokenizer(new_texts, padding=True, truncation=True, max_length=128, return_tensors=\"tf\")\n",
    "\n",
    "# 예측 수행\n",
    "predictions = model.predict(new_data[\"input_ids\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
